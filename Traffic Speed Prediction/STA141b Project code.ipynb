{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Road Speed with DiDi Data: STA 141B Final Project\n",
    "### Group 28: Saurabh Maheshwari & Sarah Grajdura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import sqlalchemy as sqla\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import time\n",
    "import eviltransform\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import geopy.distance\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "import math\n",
    "%matplotlib inline\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Conv1D, Dropout, MaxPooling1D, Flatten, Reshape, Activation, BatchNormalization, TimeDistributed, Bidirectional, Conv2D\n",
    "from keras.backend import expand_dims\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam \n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change unix time to china time\n",
    "def unix_to_china(time_stamp, from_zone = tz.gettz('UTC'), to_zone = tz.gettz('Asia/Shanghai')):\n",
    "    '''\n",
    "    arguments: \n",
    "    time_stamp: unix time stamp\n",
    "    from_zone, to_zone\n",
    "    \n",
    "    output:\n",
    "    china time\n",
    "    '''\n",
    "    u = datetime.utcfromtimestamp(int(time_stamp))\n",
    "    u = u.replace(tzinfo=from_zone)\n",
    "    u = u.astimezone(to_zone)\n",
    "    return({'month': u.month, 'day': u.day, 'hour': u.hour, 'minute': u.minute, 'second': u.second})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change china time to unix time\n",
    "def china_to_unix(month, day, hour, minute, year = 2016, second = 59, to_zone = tz.gettz('Asia/Shanghai')):\n",
    "    '''\n",
    "    arguments:\n",
    "    china time - month, day, hour, minute, year, second\n",
    "    to_zone: converting from which zone?\n",
    "    \n",
    "    output:\n",
    "    unix time\n",
    "    '''\n",
    "    \n",
    "    u = datetime.timestamp(datetime(year,month,day,hour,minute,second).replace(tzinfo = to_zone))\n",
    "    return(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transfer data to sql\n",
    "def data_to_sql(filepath, tabname, conn, chunksize = 5000000):\n",
    "    '''\n",
    "    arguments:\n",
    "    filepath: file destination (sql)\n",
    "    tabname: table name\n",
    "    conn: connection to the database\n",
    "    chunksize: chunk size to be read from the data frame\n",
    "    \n",
    "    output:\n",
    "    Data is read to sql database\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    chunks = pd.read_csv(filepath, chunksize=chunksize, header=None, \n",
    "                     names = ['driver_id', 'trip_id', 'timestamp', 'longitude', 'latitude'])\n",
    "    print('File: ', filepath)\n",
    "    #print('working on chunk: ', 1)\n",
    "    chunk = next(chunks)\n",
    "    chunk.to_sql(tabname, conn, if_exists='replace', index = False)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print('working on chunk: ', i+2)\n",
    "        chunk.to_sql(tabname, conn, if_exists='append', index = False)\n",
    "    print('time taken to read file: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map the data\n",
    "def track_driver(df, transform = True):\n",
    "    '''\n",
    "    arguments:\n",
    "    df: dataframe consisting of coordinates of driver \n",
    "    transform: whether to transfrom from gcj to wgs system of coordinates?\n",
    "    \n",
    "    output:\n",
    "    folium map\n",
    "    '''\n",
    "    driver_map = folium.Map(location=[34.2324012260476, 108.94615156073496],\n",
    "                            zoom_start = 15)\n",
    "    if transform:\n",
    "        circle_data = [eviltransform.gcj2wgs(row.latitude, row.longitude) for row in df.itertuples()]\n",
    "    else: \n",
    "        circle_data = [[row.latitude, row.longitude] for row in df.itertuples()]\n",
    "    for i in range(len(circle_data)):\n",
    "        s = str('t:'+ str(df['timestamp'].iloc[i]) +'; lon:' + str(df['longitude'].iloc[i]))\n",
    "        folium.CircleMarker(location = circle_data[i], radius = 3, tooltip=s).add_to(driver_map)\n",
    "    return(driver_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate average speed with respect to direction for a given time interval\n",
    "def speed_calc_time(df):\n",
    "    '''\n",
    "    argument:\n",
    "    df: data frame for which speeds are to be calculated\n",
    "    \n",
    "    output: \n",
    "    average speeds\n",
    "    '''\n",
    "    speed_n = []\n",
    "    speed_s = []\n",
    "    unique_trip_ids = df.trip_id.unique()\n",
    "    for i in unique_trip_ids:\n",
    "        trip_df = df[df['trip_id'] == i]\n",
    "        trip_n = trip_df[(trip_df['longitude']>=108.9468) & (trip_df['longitude']<108.94723)].sort_values('timestamp')\n",
    "        speed_n.append(trip_speed(trip_n))\n",
    "        trip_s = trip_df[trip_df['longitude']<=108.9468].sort_values('timestamp')\n",
    "        speed_s.append(trip_speed(trip_s))\n",
    "    return(np.nanmean(speed_n), np.nanmean(speed_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate distance\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    '''\n",
    "    arguments:\n",
    "    lat1, lon1: coordinates of the first position\n",
    "    lat2, lon2: coordinates of the second position\n",
    "    \n",
    "    output:\n",
    "    distance between 2 points\n",
    "    '''\n",
    "    earth_radius = 6371*1000  # m\n",
    "    dlat = math.radians(lat2-lat1)\n",
    "    dlon = math.radians(lon2-lon1)\n",
    "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    d = earth_radius * c\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate average speed given the df containing 2 sets of coordinates\n",
    "def speed_calc(df):\n",
    "    '''\n",
    "    Given dataframe (only 2 rows), calculate the average speed\n",
    "    '''\n",
    "    delta_t = (df['timestamp'].iloc[1] - df['timestamp'].iloc[0])/3600\n",
    "    lat_init, lon_init = df['latitude'].iloc[0], df['longitude'].iloc[0]\n",
    "    lat_fin, lon_fin = df['latitude'].iloc[1], df['longitude'].iloc[1]\n",
    "    dis = calculate_distance(lat_init, lon_init, lat_fin, lon_fin)/1000/delta_t\n",
    "    return(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate average speed given the df containing bunch of sets of coordinates sorted by timestamp\n",
    "def trip_speed(df):\n",
    "    '''\n",
    "    Calculate average speed given the df containing bunch of sets of coordinates sorted by timestamp\n",
    "    '''\n",
    "    l = []\n",
    "    try:\n",
    "        for i in np.arange(2, df.shape[0]+1):\n",
    "            l.append(speed_calc(df[(i-2):i]))\n",
    "    except:\n",
    "        l = np.NaN\n",
    "    #print(l)\n",
    "    return(np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To be used for approach 1\n",
    "def create_sets(df_list, window = 24, pred_step = 3, div = 80, jump = 1):\n",
    "    '''\n",
    "    arguments:\n",
    "    df_list: time series data\n",
    "    div: number used to scale the speeds\n",
    "    jump: how much to slide the window\n",
    "    window, pred_step: window length, prediction length\n",
    "    \n",
    "    output:\n",
    "    predictors and output array\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    i = 0\n",
    "    while i < (len(df_list) - pred_step - window + 1):\n",
    "        X.append(df_list[i:(i+window)])\n",
    "        y.append(df_list[(i+window):(i+window+pred_step)])\n",
    "        i += jump\n",
    "    X = pd.DataFrame(X).values/div\n",
    "    y = pd.DataFrame(y).values/div\n",
    "    y = y.reshape(y.shape[0], y.shape[1], 1)\n",
    "    return(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to be used for approach 1\n",
    "def data_creation_pipeline(window = 48, pred_step = 1, div = 80, val = ['11_28', '11_29'], test = ['11_30']):    \n",
    "    '''\n",
    "    arguments:\n",
    "    window, pred_step, div as already defined\n",
    "    val: days for validation set\n",
    "    test: days for test set\n",
    "    \n",
    "    output:\n",
    "    training, validation and test sets\n",
    "    '''\n",
    "    ## Fill NAs with the column mean\n",
    "    north_df = pd.DataFrame(north_bound)\n",
    "    south_df = pd.DataFrame(south_bound)\n",
    "    north_df.fillna(north_df.mean(), inplace = True)\n",
    "    south_df.fillna(south_df.mean(), inplace = True)\n",
    "\n",
    "    cols = []\n",
    "    for i,j in itertools.product([10,11], np.arange(1,31)):\n",
    "        cols.append(str(i)+'_'+str(j))\n",
    "\n",
    "    north_df = north_df[cols]\n",
    "    south_df = south_df[cols]\n",
    "\n",
    "    north_list_train = north_df.drop(val+test, axis = 1).values.T.reshape((1,-1)).tolist()\n",
    "    south_list_train = south_df.drop(val+test, axis = 1).values.T.reshape((1,-1)).tolist()\n",
    "    north_list_val = north_df[val].values.T.reshape((1,-1)).tolist()\n",
    "    south_list_val = south_df[val].values.T.reshape((1,-1)).tolist()\n",
    "    north_list_test = north_df[test].values.T.reshape((1,-1)).tolist()\n",
    "    south_list_test = south_df[test].values.T.reshape((1,-1)).tolist()\n",
    "\n",
    "    north_X_train, north_y_train = create_sets(north_list_train[0], window = window, pred_step = pred_step, div = div)\n",
    "    south_X_train, south_y_train = create_sets(south_list_train[0], window = window, pred_step = pred_step, div = div)\n",
    "    north_X_val, north_y_val = create_sets(north_list_val[0], window = window, pred_step = pred_step, div = div, jump = pred_step)\n",
    "    south_X_val, south_y_val = create_sets(south_list_val[0], window = window, pred_step = pred_step, div = div, jump = pred_step)\n",
    "    north_X_test, north_y_test = create_sets(north_list_test[0], window = window, pred_step = pred_step, div = div, jump = pred_step)\n",
    "    south_X_test, south_y_test = create_sets(south_list_test[0], window = window, pred_step = pred_step, div = div, jump = pred_step)\n",
    "\n",
    "    X_train = np.concatenate([north_X_train, south_X_train])\n",
    "    X_val = np.concatenate([north_X_val, south_X_val])\n",
    "    X_test = np.concatenate([north_X_test, south_X_test])\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    y_train = np.concatenate([north_y_train, south_y_train])\n",
    "    y_val = np.concatenate([north_y_val, south_y_val])\n",
    "    y_test = np.concatenate([north_y_test, south_y_test])\n",
    "\n",
    "    return(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to be used for approach 2\n",
    "def create_sets_2(df_list, window1, window2, pred_step = 3, div = 80, jump = 1):\n",
    "    '''\n",
    "    arguments:\n",
    "    df_list: time series data\n",
    "    div: number used to scale the speeds\n",
    "    jump: how much to slide the window\n",
    "    window1, window2, pred_step: window lengths, prediction length\n",
    "    \n",
    "    output:\n",
    "    predictors and output array\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    i = 0\n",
    "    while i < (len(df_list) - pred_step - window1 - window2 + 1):\n",
    "        X.append(df_list[(i):(i+window1)]+df_list[(i+window1+pred_step):(i+window1+pred_step+window2)])\n",
    "        y.append(df_list[(i+window1):(i+window1+pred_step)])\n",
    "        i += jump\n",
    "    X = pd.DataFrame(X).values/div\n",
    "    y = pd.DataFrame(y).values/div\n",
    "    y = y.reshape(y.shape[0], y.shape[1], 1)\n",
    "    return(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To be used for approach 2\n",
    "def train_val_test(df1, df2, div, div_n, div_s, pred_step, val, test, window1, window2, flow = True):\n",
    "  '''\n",
    "  arguments:\n",
    "  df1, df2: north and south speed or flow data sets\n",
    "  div: scaling number for speeds\n",
    "  div_n, div_s: scaling factor for north and south bound flows\n",
    "  pred_step, window1, window2: prediction length, window lengths\n",
    "  val, test: validation and test days\n",
    "  flow: whether df1, df2 flow datasets?\n",
    "  \n",
    "  output:\n",
    "  training, validation and test data sets\n",
    "  '''\n",
    "    north_list_train = df1.drop(val+test, axis = 1).values.T.reshape((1,-1)).tolist()\n",
    "    south_list_train = df2.drop(val+test, axis = 1).values.T.reshape((1,-1)).tolist()\n",
    "    north_list_val = df1[val].values.T.reshape((1,-1)).tolist()\n",
    "    south_list_val = df2[val].values.T.reshape((1,-1)).tolist()\n",
    "    north_list_test = df1[test].values.T.reshape((1,-1)).tolist()\n",
    "    south_list_test = df2[test].values.T.reshape((1,-1)).tolist()\n",
    "\n",
    "    if (flow):\n",
    "        north_X_train, north_y_train = create_sets_2(north_list_train[0], window1, window2, pred_step = pred_step, div = div_n)\n",
    "        south_X_train, south_y_train = create_sets_2(south_list_train[0], window1, window2, pred_step = pred_step, div = div_s)\n",
    "        north_X_val, north_y_val = create_sets_2(north_list_val[0], window1, window2, pred_step = pred_step, \n",
    "                                                 div = div_n, jump = pred_step)\n",
    "        south_X_val, south_y_val = create_sets_2(south_list_val[0], window1, window2, pred_step = pred_step, \n",
    "                                                 div = div_s, jump = pred_step)\n",
    "        north_X_test, north_y_test = create_sets_2(north_list_test[0], window1, window2, pred_step = pred_step, \n",
    "                                                   div = div_n, jump = pred_step)\n",
    "        south_X_test, south_y_test = create_sets_2(south_list_test[0], window1, window2, pred_step = pred_step, \n",
    "                                                   div = div_s, jump = pred_step)\n",
    "    else:\n",
    "        north_X_train, north_y_train = create_sets_2(north_list_train[0], window1, window2, pred_step = pred_step, div = div)\n",
    "        south_X_train, south_y_train = create_sets_2(south_list_train[0], window1, window2, pred_step = pred_step, div = div)\n",
    "        north_X_val, north_y_val = create_sets_2(north_list_val[0], window1, window2, pred_step = pred_step, \n",
    "                                                 div = div, jump = pred_step)\n",
    "        south_X_val, south_y_val = create_sets_2(south_list_val[0], window1, window2, pred_step = pred_step, \n",
    "                                                 div = div, jump = pred_step)\n",
    "        north_X_test, north_y_test = create_sets_2(north_list_test[0], window1, window2, pred_step = pred_step, \n",
    "                                                   div = div, jump = pred_step)\n",
    "        south_X_test, south_y_test = create_sets_2(south_list_test[0], window1, window2, pred_step = pred_step, \n",
    "                                                   div = div, jump = pred_step)\n",
    "\n",
    "    X_train_data = np.concatenate([north_X_train, south_X_train])\n",
    "    X_val_data = np.concatenate([north_X_val, south_X_val])\n",
    "    X_test_data = np.concatenate([north_X_test, south_X_test])\n",
    "\n",
    "    X_train_data = X_train_data.reshape(X_train_data.shape[0], X_train_data.shape[1], 1)\n",
    "    X_val_data = X_val_data.reshape(X_val_data.shape[0], X_val_data.shape[1], 1)\n",
    "    X_test_data = X_test_data.reshape(X_test_data.shape[0], X_test_data.shape[1], 1)\n",
    "\n",
    "    y_train = np.concatenate([north_y_train, south_y_train])\n",
    "    y_val = np.concatenate([north_y_val, south_y_val])\n",
    "    y_test = np.concatenate([north_y_test, south_y_test])\n",
    "\n",
    "    return(X_train_data, X_val_data, X_test_data, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To be used for approach 2\n",
    "def data_creation_pipeline_flow(window1 = 60, window2 = 40, pred_step = 60, div = 80, div_n = 63, div_s = 48, \n",
    "                                val = ['11_28', '11_29'], test = ['11_30']):\n",
    "    '''\n",
    "    This function basically calls train_val_test and stacks speed and flow features\n",
    "    arguments:\n",
    "    window1, window2, pred_step, div, div_n, div_s, val, test: all defined above\n",
    "    \n",
    "    output:\n",
    "    stacked speed and flow features to be used for training, validation and testing\n",
    "    '''\n",
    "    \n",
    "    north_df = pd.DataFrame(north_bound)\n",
    "    south_df = pd.DataFrame(south_bound)\n",
    "    north_df.fillna(north_df.mean(), inplace = True)\n",
    "    south_df.fillna(south_df.mean(), inplace = True)\n",
    "\n",
    "    north_df_flow = pd.DataFrame(north_bound_flow)\n",
    "    south_df_flow = pd.DataFrame(south_bound_flow)\n",
    "\n",
    "    cols = []\n",
    "    for i,j in itertools.product([10,11], np.arange(1,31)):\n",
    "        cols.append(str(i)+'_'+str(j))\n",
    "\n",
    "    north_df = north_df[cols]\n",
    "    south_df = south_df[cols]\n",
    "    north_df_flow = north_df_flow[cols]\n",
    "    south_df_flow = south_df_flow[cols]\n",
    "\n",
    "    X_train_speed, X_val_speed, X_test_speed, y_train, y_val, y_test = train_val_test(north_df, south_df, div, div_n,\n",
    "                                                                                      div_s, pred_step, val, test, \n",
    "                                                                                      window1, window2, flow = False)\n",
    "    X_train_flow, X_val_flow, X_test_flow, _, _, _ = train_val_test(north_df_flow, south_df_flow, div, div_n, div_s, \n",
    "                                                                    pred_step, val, test, window1, window2, flow = True)\n",
    "\n",
    "    X_train = np.dstack([X_train_speed, X_train_flow])\n",
    "    X_val = np.dstack([X_val_speed, X_val_flow])\n",
    "    X_test = np.dstack([X_test_speed, X_test_flow])\n",
    "\n",
    "    return(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_speed(month, day, cur, hour_init = [0], hour_final = [23]):\n",
    "    '''\n",
    "    fetch speeds from the sql database\n",
    "    \n",
    "    arguments:\n",
    "    month, day of the date\n",
    "    cur: cursor to the sql database\n",
    "    hour_init, hour_final: initial and final hours of extracting the data\n",
    "    \n",
    "    output:\n",
    "    north and south bound speeds\n",
    "    '''\n",
    "    \n",
    "    speed_n = []\n",
    "    speed_s = []\n",
    "    if len(str(day)) == 1:\n",
    "        name = str(month)+'0'+str(day)\n",
    "    else: \n",
    "        name = str(month)+str(day)\n",
    "    print(name)\n",
    "    min_init, min_final = 0, 59\n",
    "    sec_init, sec_final = 0, 59\n",
    "    for hr_init, hr_final in zip(hour_init, hour_final):\n",
    "        cur.execute('''select driver_id, trip_id, longitude, latitude, timestamp \n",
    "        from {} where timestamp between (?) and (?) and latitude between (?) and (?) \n",
    "        and longitude between (?) and (?)'''.format(\"'\"+name+\"'\"), \n",
    "                    (china_to_unix(month, day, hr_init, min_init, second=sec_init), \n",
    "                     china_to_unix(month, day, hr_final, min_final, second=sec_final),\n",
    "                     34.2324012260476, 34.23940384395769, 108.94615156073496, 108.94765571288106))\n",
    "        df = pd.DataFrame(cur.fetchall(), columns = ['driver_id', 'trip_id', 'longitude', 'latitude', 'timestamp'])\n",
    "        for cols in ['timestamp', 'latitude', 'longitude']:\n",
    "            if df[cols].dtype == 'object':\n",
    "                df[cols] = df[cols].astype('float')\n",
    "        for h,m in itertools.product(np.arange(hr_init, hr_final+1), np.arange(4,60,5)):\n",
    "            m_init, m_final = m-4, m\n",
    "            t_init = china_to_unix(month, day, h, m_init, second=sec_init)\n",
    "            t_final = china_to_unix(month, day, h, m_final, second=sec_final)\n",
    "            df_mini = df[(df['timestamp']>=t_init) & (df['timestamp']=<t_final)]\n",
    "            print(df_mini.shape)\n",
    "            speeds = speed_calc_time(df_mini)\n",
    "            speed_n.append(speeds[0])\n",
    "            speed_s.append(speeds[1])\n",
    "    return(speed_n, speed_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flow_calc_time(df):\n",
    "    '''\n",
    "    argument:\n",
    "    data frame for which the flow is to be calculated\n",
    "    \n",
    "    output:\n",
    "    north and south bound flows for the data frame\n",
    "    '''\n",
    "    trip_n = df[(df['longitude']>=108.9468)]\n",
    "    flow_n = trip_n['driver_id'].nunique()\n",
    "    trip_s = df[df['longitude']<108.9468]\n",
    "    flow_s = trip_s['driver_id'].nunique()\n",
    "    return(flow_n, flow_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_flow(month, day, cur, hour_init = [0], hour_final = [23]):\n",
    "    '''\n",
    "    fetch flow data from the sql database\n",
    "    \n",
    "    arguments:\n",
    "    month, day of the date\n",
    "    cur: cursor to the sql database\n",
    "    hour_init, hour_final: initial and final hours of extracting the data\n",
    "    \n",
    "    output:\n",
    "    north and south bound flow\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    flow_n = []\n",
    "    flow_s = []\n",
    "    if len(str(day)) == 1:\n",
    "        name = str(month)+'0'+str(day)\n",
    "    else: \n",
    "        name = str(month)+str(day)\n",
    "    print(name)\n",
    "    min_init, min_final = 0, 59\n",
    "    sec_init, sec_final = 0, 59\n",
    "    for hr_init, hr_final in zip(hour_init, hour_final):\n",
    "        cur.execute('''select driver_id, longitude, latitude, timestamp \n",
    "        from {} where timestamp between (?) and (?) and latitude between (?) and (?) \n",
    "        and longitude between (?) and (?)'''.format(\"'\"+name+\"'\"), \n",
    "                    (china_to_unix(month, day, hr_init, min_init, second=sec_init), \n",
    "                     china_to_unix(month, day, hr_final, min_final, second=sec_final),\n",
    "                     34.2324012260476, 34.23940384395769, 108.94615156073496, 108.94765571288106))\n",
    "        df = pd.DataFrame(cur.fetchall(), columns = ['driver_id', 'longitude', 'latitude', 'timestamp'])\n",
    "        for cols in ['longitude', 'latitude', 'timestamp']:\n",
    "            if df[cols].dtype == 'object':\n",
    "                df[cols] = df[cols].astype('float')\n",
    "        for h,m in itertools.product(np.arange(hr_init, hr_final+1), np.arange(4,60,5)):\n",
    "            m_init, m_final = m-4, m\n",
    "            t_init = china_to_unix(month, day, h, m_init, second=sec_init)\n",
    "            t_final = china_to_unix(month, day, h, m_final, second=sec_final)\n",
    "            df_mini = df[(df['timestamp']>=t_init) & (df['timestamp']<t_final)]\n",
    "            flow = flow_calc_time(df_mini)\n",
    "            flow_n.append(flow[0])\n",
    "            flow_s.append(flow[1])\n",
    "    return(flow_n, flow_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do predictions\n",
    "def decode_sequence(input_sequence):\n",
    "    '''\n",
    "    Do predictions for the decoder module\n",
    "    \n",
    "    argument: \n",
    "    input sequence of the time series\n",
    "    \n",
    "    output:\n",
    "    future predictions\n",
    "    '''\n",
    "    encoding_h, encoding_c = encoder_infer.predict(input_sequence)\n",
    "    target_seq = np.zeros((1,1,1))\n",
    "    target_seq[0,0,0] = input_sequence[:,-1,:]\n",
    "    decoded_seq = np.zeros((1,pred_step,1))\n",
    "    for i in range(pred_step):\n",
    "        output, h, c = decoder_infer_model.predict([target_seq, encoding_h, encoding_c])\n",
    "        decoded_seq[0, i, 0] = output\n",
    "        encoding_h = h\n",
    "        encoding_c = c\n",
    "    return(decoded_seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_ex_feature(speed, flow, north = True):\n",
    "    '''\n",
    "    create speed and flow features for the extra day file\n",
    "    \n",
    "    arguments: \n",
    "    speed, flow - speed and flow data for the extra day file\n",
    "    north - whether to create north bound features or not\n",
    "    \n",
    "    output:\n",
    "    speed and flow features for extra day file\n",
    "    '''\n",
    "    \n",
    "    # Time steps for window1 and window 2 for extra day file for north and south bound tracks\n",
    "    if north:\n",
    "        R11, L11, R12, L12, R21, L21, R22, L22 = 68, 8, 164, 128, 188, 128, 284, 248\n",
    "    else:\n",
    "        R11, L11, R12, L12, R21, L21, R22, L22 = 63, 3, 159, 123, 183, 123, 279, 243\n",
    "    speed_ex_1 = pd.concat([speed[L11:R11], speed[L12:R12]]).values\n",
    "    speed_ex_1 = speed_ex_1.reshape(1, speed_ex_1.shape[0])\n",
    "    flow_ex_1 = np.array(flow[L11:R11] + flow[L12:R12])\n",
    "    flow_ex_1 = flow_ex_1.reshape(1, flow_ex_1.shape[0])\n",
    "    \n",
    "    speed_ex_2 = pd.concat([speed[L21:R21], speed[L22:R22]]).values\n",
    "    speed_ex_2 = speed_ex_2.reshape(1, speed_ex_2.shape[0])\n",
    "    flow_ex_2 = np.array(flow[L21:R21] + flow[L22:R22])\n",
    "    flow_ex_2 = flow_ex_2.reshape(1, flow_ex_2.shape[0])\n",
    "    \n",
    "    feat1 = np.dstack([speed_ex_1, flow_ex_1])\n",
    "    feat2 = np.dstack([speed_ex_2, flow_ex_2])\n",
    "    return(feat1, feat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extra day speeds\n",
    "speed_north = pd.read_csv(r'G:\\DiDi_sql\\Predictions\\Predictions_north.csv', header = 0, na_values='x')\n",
    "speed_south = pd.read_csv(r'G:\\DiDi_sql\\Predictions\\Predictions_south.csv', header = 0, na_values='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_n = speed_north['speed']\n",
    "sp_s = speed_south['speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dates for which data is to be stored in sql database\n",
    "filepath = r'I:\\DiDi Data\\gps_2016'\n",
    "dates = np.arange(1001,1031).tolist()\n",
    "dates.extend(np.arange(1101, 1131).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connection to the sqlite database\n",
    "conn = sqlite3.connect(r'G:\\DiDi_sql\\DiDi_30days.sqlite')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put data in sql database\n",
    "for date in dates:\n",
    "    path = filepath + str(date)\n",
    "    tabname = str(date)\n",
    "    data_to_sql(path, tabname, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract speeds\n",
    "north_bound = {}\n",
    "south_bound = {}\n",
    "start = time.time()\n",
    "for month, day in itertools.product([10,11], np.arange(1,31)):\n",
    "    speeds = fetch_speed(month, day, cur)\n",
    "    north_bound[str(month)+'_'+str(day)] = speeds[0]\n",
    "    south_bound[str(month)+'_'+str(day)] = speeds[1]\n",
    "print('time taken: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save north bound speeds \n",
    "with open('north_bound.pickle', 'wb') as file:\n",
    "    pickle.dump(north_bound, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save south bound speeds\n",
    "with open('south_bound.pickle', 'wb') as file:\n",
    "    pickle.dump(south_bound, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract north and south bound flows for north and south directions\n",
    "north_bound_flow = {}\n",
    "south_bound_flow = {}\n",
    "start = time.time()\n",
    "for month, day in itertools.product([10,11], np.arange(1,31)):\n",
    "    flow = fetch_flow(month, day, cur)\n",
    "    north_bound_flow[str(month)+'_'+str(day)] = flow[0]\n",
    "    south_bound_flow[str(month)+'_'+str(day)] = flow[1]\n",
    "print('time taken: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save north bound flows\n",
    "with open('north_bound_flow.pickle', 'wb') as file:\n",
    "    pickle.dump(north_bound_flow, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save south bound flows\n",
    "with open('south_bound_flow.pickle', 'wb') as file:\n",
    "    pickle.dump(south_bound_flow, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fetch flow information for extra day file\n",
    "flow = fetch_flow(12, 1, cur)\n",
    "north_flow_ex = flow[0]\n",
    "south_flow_ex = flow[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create features for the extra day\n",
    "north1_feat, north2_feat = create_ex_feature(sp_n, north_flow_ex, north = True)\n",
    "south1_feat, south2_feat = create_ex_feature(sp_s, south_flow_ex, north = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save extra day features\n",
    "with open('extra_day_sp_flow.pickle', 'wb') as file:\n",
    "    pickle.dump([north1_feat, north2_feat, south1_feat, south2_feat], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1 - Create Encoder - Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the required data\n",
    "with open('north_bound.pickle', 'rb') as file:\n",
    "    north_bound = pickle.load(file)\n",
    "with open('south_bound.pickle', 'rb') as file:\n",
    "    south_bound = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training, validation and test sets\n",
    "window = 72\n",
    "pred_step = 2\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = data_creation_pipeline(window = window, pred_step=pred_step, div = 80)\n",
    "# Generate input for encoder_decoder network training\n",
    "encoder_input = X_train\n",
    "decoder_output = y_train\n",
    "decoder_input = np.zeros(decoder_output.shape)\n",
    "decoder_input[:,1:,:] = y_train[:,:-1,:]\n",
    "decoder_input[:,0,:] = X_train[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Develop encoder decoder module\n",
    "\n",
    "# 1D-CNN part\n",
    "seed(1)\n",
    "set_random_seed(2)\n",
    "dilation_rates = [2**i for i in range(6)]\n",
    "encoder_inp = Input((None, 1))\n",
    "x = encoder_inp\n",
    "for dilation_rate in dilation_rates:\n",
    "    x = Conv1D(filters=128,\n",
    "               kernel_size=3,\n",
    "               padding='causal',\n",
    "               dilation_rate=dilation_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "x = MaxPooling1D(pool_size = 2, strides = 2)(x)\n",
    "x = TimeDistributed(Dense(128))(x)\n",
    "\n",
    "# training encoder_decoder\n",
    "# encoder part\n",
    "cells = 256\n",
    "x = LSTM(cells, return_sequences=True)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "_, encoder_h, encoder_c = LSTM(cells, return_state = True)(x)\n",
    "encoder_out = [encoder_h, encoder_c]\n",
    "\n",
    "# decoder part \n",
    "decoder_inp = Input((None, 1))\n",
    "decoder_lstm = LSTM(cells, return_sequences=True, return_state=True)\n",
    "x, _, _ = decoder_lstm(decoder_inp, initial_state = encoder_out)\n",
    "decoder_dense = Dense(1)\n",
    "x = decoder_dense(x)\n",
    "x_out = x\n",
    "\n",
    "# train model\n",
    "encoder_decoder_model = Model(inputs = [encoder_inp, decoder_inp], outputs = x_out)\n",
    "adam = Adam(0.0001)\n",
    "encoder_decoder_model.compile(adam, 'mean_squared_error')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True)\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "seed(124)\n",
    "print('training encoder_decoder_model...')\n",
    "encoder_decoder_model.fit(x = [encoder_input, decoder_input], y = decoder_output, validation_split=0.2, epochs=n_epochs, batch_size=batch_size, \n",
    "                         callbacks = [early_stopping])\n",
    "\n",
    "# encoder_decoder_inference part\n",
    "# get encodings\n",
    "encoder_infer = Model(encoder_inp, encoder_out)\n",
    "# decoder inference part\n",
    "decoder_infer_h = Input((cells, ))\n",
    "decoder_infer_c = Input((cells, ))\n",
    "decoder_infer_input = [decoder_infer_h, decoder_infer_c]\n",
    "# use same decoder LSTM and dense layers as used while training \n",
    "decoder_infer_output, state_h, state_c = decoder_lstm(decoder_inp, initial_state = decoder_infer_input) \n",
    "decoder_infer_states = [state_h, state_c]\n",
    "decoder_infer_output = decoder_dense(decoder_infer_output)\n",
    "decoder_infer_model = Model([decoder_inp] + decoder_infer_input, [decoder_infer_output]+decoder_infer_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create predictions for validation set\n",
    "pred_val = np.zeros(y_val.shape)\n",
    "for i in range(X_val.shape[0]):\n",
    "    pred_val[i,:,:] = decode_sequence(X_val[(i):(i+1),:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create inference for continuous 60 steps in the test set\n",
    "test = pd.DataFrame(north_bound['11_30'])/80\n",
    "test.fillna(test.mean(), inplace = True)\n",
    "test = test.values\n",
    "X_test_infer = test[:window, :]\n",
    "X_test_infer_re = X_test_infer.reshape((1, window, 1))\n",
    "pred_infer = decode_sequence(X_test_infer_re)*80\n",
    "pred_infer_re = pred_infer.reshape(pred_step, 1)\n",
    "for i in range(30):\n",
    "    X_test_infer = np.concatenate([X_test_infer[pred_step:, :], pred_infer_re], axis = 0)\n",
    "    X_test_infer_re = X_test_infer.reshape((1, window, 1))\n",
    "    pred_infer_re = decode_sequence(X_test_infer_re)*80\n",
    "    pred_infer = np.concatenate([pred_infer, pred_infer_re], axis = 0)\n",
    "    pred_infer_re = pred_infer_re.reshape(pred_step, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: Bi-Directional CNN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the required data\n",
    "with open('north_bound_flow.pickle', 'rb') as file:\n",
    "    north_bound_flow = pickle.load(file)\n",
    "with open('south_bound_flow.pickle', 'rb') as file:\n",
    "    south_bound_flow = pickle.load(file)\n",
    "with open('north_bound.pickle', 'rb') as file:\n",
    "    north_bound = pickle.load(file)\n",
    "with open('south_bound.pickle', 'rb') as file:\n",
    "    south_bound = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Develop bi-directional CNN-LSTM model\n",
    "seed(1)\n",
    "set_random_seed(2)\n",
    "# 1-D CNN part\n",
    "dilation_rates = [2**i for i in range(6)]\n",
    "encoder_inp = Input((window1+window2, 2))\n",
    "x = encoder_inp\n",
    "for dilation_rate in dilation_rates:\n",
    "    x = Conv1D(filters=128,\n",
    "               kernel_size=3,\n",
    "               padding='same',\n",
    "               dilation_rate=dilation_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "x = MaxPooling1D(pool_size = 2, strides = 2)(x)\n",
    "x = TimeDistributed(Dense(128))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "# Bi-Directional LSTM part\n",
    "cells = 256\n",
    "x = Bidirectional(LSTM(cells))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(pred_step)(x)\n",
    "x = Reshape((pred_step, 1))(x)\n",
    "x_out = x\n",
    "model = Model(inputs = [encoder_inp], outputs = [x_out])\n",
    "model.summary()\n",
    "# Model fitting\n",
    "model.compile('adam', 'mean_squared_error')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True)\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "seed(124)\n",
    "model.fit(X_train, y_train, validation_split=0.2, callbacks=[early_stopping], batch_size=batch_size, epochs = n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get predictions on validation and test sets\n",
    "pred_val = model.predict(X_val)\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions for extra day file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load extra day flow\n",
    "with open('extra_day_sp_flow.pickle', 'rb') as file:\n",
    "    extra_feat = pickle.load(file)\n",
    "north1_feat, north2_feat, south1_feat, south2_feat = extra_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions using CNN-LSTM bi-directional model\n",
    "extra_n_1_pred = model.predict(north1_feat/80)*80\n",
    "extra_n_2_pred = model.predict(north2_feat/80)*80\n",
    "extra_s_1_pred = model.predict(south1_feat/80)*80\n",
    "extra_s_2_pred = model.predict(south2_feat/80)*80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill the missing speeds in the north speed prediction file\n",
    "sp_n_pred = sp_n.copy()\n",
    "sp_n_pred[sp_n_pred.isnull()] = np.concatenate([extra_n_1_pred.flatten(), extra_n_2_pred.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill the missing speeds in the south speed prediction file\n",
    "sp_s_pred = sp_s.copy()\n",
    "sp_s_pred[sp_s_pred.isnull()] = np.concatenate([extra_s_1_pred.flatten(), extra_s_2_pred.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save north speed prediction file\n",
    "prediction_north = pd.Series(index=speed_north.iloc[:,0], data = sp_n_pred.values)\n",
    "prediction_north.to_csv('prediction_north.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save south speed prediction file\n",
    "prediction_south = pd.Series(index = speed_south.iloc[:,0], data = sp_s_pred.values)\n",
    "prediction_south.to_csv('prediction_south.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
