{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "from matplotlib import rc\n",
    "rc('figure', figsize=(15, 5))\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from timeit import default_timer as timer\n",
    "import pickle\n",
    "import hyperopt\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "import seaborn as sns\n",
    "from timeit import default_timer as timer\n",
    "from numpy.random import RandomState\n",
    "from hyperopt import STATUS_OK\n",
    "import csv\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test = pd.read_csv(r'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test(test):\n",
    "    test = test.loc[(test['shop_id'].isin(sales_test['shop_id']))&(test['item_id'].isin(sales_test['item_id'])),:].copy()\n",
    "    test_pred = test['cnt_shop_item'].copy()\n",
    "    test.drop('cnt_shop_item', axis = 1, inplace = True)\n",
    "    return(test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cv_sets2(months, data):\n",
    "    X_train = data.loc[~data['date_block_num'].isin(months+[34]),:].drop('cnt_shop_item', axis=1)\n",
    "    X_val = data.loc[data['date_block_num'].isin(months),:]\n",
    "    X_val, y_val = create_test(X_val)\n",
    "    X_test = data.loc[data['date_block_num'] == 34,:].drop('cnt_shop_item',axis=1)\n",
    "    y_train = data.loc[~data['date_block_num'].isin(months+[34]),'cnt_shop_item']\n",
    "    return(dict({'train': X_train, 'val': X_val, 'test': X_test, 'train_y': y_train, 'val_y': y_val}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(prediction, dict_cat):\n",
    "  trtv = pd.concat([dict_cat['train'], dict_cat['val']], axis = 0)\n",
    "  test_ids = dict_cat['test']['item_id'].isin(trtv['item_id']) & dict_cat['test']['shop_id'].isin(trtv['shop_id'])\n",
    "  prediction[~test_ids] = 0\n",
    "  return(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(pred, act):\n",
    "    return(np.sqrt(np.mean(np.square(np.subtract(act,pred)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"full_data_2.pkl\", \"rb\") as input_file:\n",
    "    full_data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cat = create_cv_sets2([9,21,33], full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    global ITERATION\n",
    "    ITERATION +=1\n",
    "    print('ITERATION: %d' %(ITERATION))\n",
    "    print('params: ', params)\n",
    "    start = timer()\n",
    "    rf = RandomForestRegressor(n_estimators = 50, max_depth = params['max_depth'], max_features = params['max_features'], \n",
    "                               min_samples_leaf=params['min_samples_leaf'], n_jobs = -1)\n",
    "    rf.fit(dict_cat['train'], dict_cat['train_y'])\n",
    "    loss = error(rf.predict(dict_cat['val']), dict_cat['val_y'])\n",
    "    train_time = timer()-start\n",
    "    print('loss: %.5f' %(loss))\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([ITERATION, loss, params, train_time])\n",
    "    return {'iteration': ITERATION, 'loss': loss, 'params': params,   \n",
    "            'train_time': train_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_space = {\n",
    "    'max_depth': hp.choice('subsample', np.arange(1,11,1, dtype = int)),\n",
    "    'max_features': hp.quniform('max_features', 0.05, 1, 0.05),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', np.arange(10, 101, 5, dtype = int))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9, 'max_features': 0.5, 'min_samples_leaf': 95}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(params_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'rf_trials_pfs4.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['iteration', 'loss', 'params', 'train_time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION: 1\n",
      "params:  {'max_depth': 4, 'max_features': 0.35000000000000003, 'min_samples_leaf': 100}\n",
      "loss: 1.95021\n",
      "ITERATION: 2\n",
      "params:  {'max_depth': 4, 'max_features': 0.25, 'min_samples_leaf': 10}\n",
      "loss: 1.98292\n",
      "ITERATION: 3\n",
      "params:  {'max_depth': 4, 'max_features': 0.9500000000000001, 'min_samples_leaf': 80}\n",
      "loss: 1.98167\n",
      "ITERATION: 4\n",
      "params:  {'max_depth': 5, 'max_features': 0.1, 'min_samples_leaf': 35}\n",
      "loss: 2.03247\n",
      "ITERATION: 5\n",
      "params:  {'max_depth': 2, 'max_features': 0.30000000000000004, 'min_samples_leaf': 55}\n",
      "loss: 2.20559\n",
      "ITERATION: 6\n",
      "params:  {'max_depth': 2, 'max_features': 0.45, 'min_samples_leaf': 100}\n",
      "loss: 2.16243\n",
      "ITERATION: 7\n",
      "params:  {'max_depth': 3, 'max_features': 0.45, 'min_samples_leaf': 90}\n",
      "loss: 2.03427\n",
      "ITERATION: 8\n",
      "params:  {'max_depth': 5, 'max_features': 0.75, 'min_samples_leaf': 70}\n",
      "loss: 1.89057\n",
      "ITERATION: 9\n",
      "params:  {'max_depth': 8, 'max_features': 0.55, 'min_samples_leaf': 10}\n",
      "loss: 1.72147\n",
      "ITERATION: 10\n",
      "params:  {'max_depth': 3, 'max_features': 0.8, 'min_samples_leaf': 20}\n",
      "loss: 2.05568\n",
      "ITERATION: 11\n",
      "params:  {'max_depth': 4, 'max_features': 0.1, 'min_samples_leaf': 80}\n",
      "loss: 2.11036\n",
      "ITERATION: 12\n",
      "params:  {'max_depth': 6, 'max_features': 0.9, 'min_samples_leaf': 65}\n",
      "loss: 1.82226\n",
      "ITERATION: 13\n",
      "params:  {'max_depth': 8, 'max_features': 0.65, 'min_samples_leaf': 15}\n",
      "loss: 1.72156\n",
      "ITERATION: 14\n",
      "params:  {'max_depth': 8, 'max_features': 0.7000000000000001, 'min_samples_leaf': 70}\n",
      "loss: 1.72371\n",
      "ITERATION: 15\n",
      "params:  {'max_depth': 4, 'max_features': 0.9500000000000001, 'min_samples_leaf': 10}\n",
      "loss: 1.97896\n",
      "ITERATION: 16\n",
      "params:  {'max_depth': 8, 'max_features': 0.6000000000000001, 'min_samples_leaf': 65}\n",
      "loss: 1.72196\n",
      "ITERATION: 17\n",
      "params:  {'max_depth': 3, 'max_features': 0.1, 'min_samples_leaf': 85}\n",
      "loss: 2.22243\n",
      "ITERATION: 18\n",
      "params:  {'max_depth': 1, 'max_features': 0.2, 'min_samples_leaf': 75}\n",
      "loss: 2.48091\n",
      "ITERATION: 19\n",
      "params:  {'max_depth': 5, 'max_features': 0.15000000000000002, 'min_samples_leaf': 40}\n",
      "loss: 1.96522\n",
      "ITERATION: 20\n",
      "params:  {'max_depth': 5, 'max_features': 0.30000000000000004, 'min_samples_leaf': 50}\n",
      "loss: 1.89004\n",
      "ITERATION: 21\n",
      "params:  {'max_depth': 8, 'max_features': 0.6000000000000001, 'min_samples_leaf': 15}\n",
      "loss: 1.71774\n",
      "ITERATION: 22\n",
      "params:  {'max_depth': 10, 'max_features': 0.55, 'min_samples_leaf': 60}\n",
      "loss: 1.66292\n",
      "ITERATION: 23\n",
      "params:  {'max_depth': 10, 'max_features': 0.45, 'min_samples_leaf': 15}\n",
      "loss: 1.65877\n",
      "ITERATION: 24\n",
      "params:  {'max_depth': 10, 'max_features': 0.45, 'min_samples_leaf': 60}\n",
      "loss: 1.66479\n",
      "ITERATION: 25\n",
      "params:  {'max_depth': 10, 'max_features': 0.4, 'min_samples_leaf': 45}\n",
      "loss: 1.66495\n",
      "ITERATION: 26\n",
      "params:  {'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 60}\n",
      "loss: 1.66571\n",
      "ITERATION: 27\n",
      "params:  {'max_depth': 7, 'max_features': 0.8500000000000001, 'min_samples_leaf': 95}\n",
      "loss: 1.76894\n",
      "ITERATION: 28\n",
      "params:  {'max_depth': 9, 'max_features': 0.55, 'min_samples_leaf': 30}\n",
      "loss: 1.68260\n",
      "ITERATION: 29\n",
      "params:  {'max_depth': 10, 'max_features': 0.35000000000000003, 'min_samples_leaf': 60}\n",
      "loss: 1.67554\n",
      "ITERATION: 30\n",
      "params:  {'max_depth': 10, 'max_features': 0.7000000000000001, 'min_samples_leaf': 25}\n",
      "loss: 1.65561\n",
      "ITERATION: 31\n",
      "params:  {'max_depth': 1, 'max_features': 0.7000000000000001, 'min_samples_leaf': 25}\n",
      "loss: 2.36418\n",
      "ITERATION: 32\n",
      "params:  {'max_depth': 10, 'max_features': 0.75, 'min_samples_leaf': 25}\n",
      "loss: 1.65249\n",
      "ITERATION: 33\n",
      "params:  {'max_depth': 6, 'max_features': 0.8, 'min_samples_leaf': 25}\n",
      "loss: 1.81682\n",
      "ITERATION: 34\n",
      "params:  {'max_depth': 7, 'max_features': 0.9, 'min_samples_leaf': 25}\n",
      "loss: 1.77112\n",
      "ITERATION: 35\n",
      "params:  {'max_depth': 10, 'max_features': 1.0, 'min_samples_leaf': 25}\n",
      "loss: 1.65704\n",
      "ITERATION: 36\n",
      "params:  {'max_depth': 9, 'max_features': 0.75, 'min_samples_leaf': 35}\n",
      "loss: 1.68359\n",
      "ITERATION: 37\n",
      "params:  {'max_depth': 2, 'max_features': 0.65, 'min_samples_leaf': 55}\n",
      "loss: 2.14603\n",
      "ITERATION: 38\n",
      "params:  {'max_depth': 10, 'max_features': 0.8500000000000001, 'min_samples_leaf': 25}\n",
      "loss: 1.65672\n",
      "ITERATION: 39\n",
      "params:  {'max_depth': 2, 'max_features': 0.7000000000000001, 'min_samples_leaf': 100}\n",
      "loss: 2.14452\n",
      "ITERATION: 40\n",
      "params:  {'max_depth': 10, 'max_features': 0.8, 'min_samples_leaf': 20}\n",
      "loss: 1.65576\n",
      "ITERATION: 41\n",
      "params:  {'max_depth': 6, 'max_features': 1.0, 'min_samples_leaf': 90}\n",
      "loss: 1.83274\n",
      "ITERATION: 42\n",
      "params:  {'max_depth': 7, 'max_features': 0.75, 'min_samples_leaf': 25}\n",
      "loss: 1.76588\n",
      "ITERATION: 43\n",
      "params:  {'max_depth': 9, 'max_features': 0.9500000000000001, 'min_samples_leaf': 80}\n",
      "loss: 1.69112\n",
      "ITERATION: 44\n",
      "params:  {'max_depth': 4, 'max_features': 0.65, 'min_samples_leaf': 75}\n",
      "loss: 1.95751\n",
      "ITERATION: 45\n",
      "params:  {'max_depth': 3, 'max_features': 0.5, 'min_samples_leaf': 50}\n",
      "loss: 2.03485\n",
      "ITERATION: 46\n",
      "params:  {'max_depth': 1, 'max_features': 0.8500000000000001, 'min_samples_leaf': 45}\n",
      "loss: 2.36169\n",
      "ITERATION: 47\n",
      "params:  {'max_depth': 10, 'max_features': 0.9, 'min_samples_leaf': 85}\n",
      "loss: 1.66994\n",
      "ITERATION: 48\n",
      "params:  {'max_depth': 4, 'max_features': 0.6000000000000001, 'min_samples_leaf': 95}\n",
      "loss: 1.95838\n",
      "ITERATION: 49\n",
      "params:  {'max_depth': 5, 'max_features': 0.75, 'min_samples_leaf': 70}\n",
      "loss: 1.88187\n",
      "ITERATION: 50\n",
      "params:  {'max_depth': 2, 'max_features': 0.4, 'min_samples_leaf': 40}\n",
      "loss: 2.16716\n",
      "ITERATION: 51\n",
      "params:  {'max_depth': 3, 'max_features': 0.7000000000000001, 'min_samples_leaf': 30}\n",
      "loss: 2.04997\n",
      "ITERATION: 52\n",
      "params:  {'max_depth': 6, 'max_features': 0.65, 'min_samples_leaf': 10}\n",
      "loss: 1.81352\n",
      "ITERATION: 53\n",
      "params:  {'max_depth': 10, 'max_features': 0.25, 'min_samples_leaf': 65}\n",
      "loss: 1.68499\n",
      "ITERATION: 54\n",
      "params:  {'max_depth': 1, 'max_features': 0.9500000000000001, 'min_samples_leaf': 35}\n",
      "loss: 2.36930\n",
      "ITERATION: 55\n",
      "params:  {'max_depth': 8, 'max_features': 0.5, 'min_samples_leaf': 55}\n",
      "loss: 1.72500\n",
      "ITERATION: 56\n",
      "params:  {'max_depth': 10, 'max_features': 0.8, 'min_samples_leaf': 100}\n",
      "loss: 1.67148\n",
      "ITERATION: 57\n",
      "params:  {'max_depth': 5, 'max_features': 0.6000000000000001, 'min_samples_leaf': 90}\n",
      "loss: 1.87662\n",
      "ITERATION: 58\n",
      "params:  {'max_depth': 7, 'max_features': 0.8500000000000001, 'min_samples_leaf': 20}\n",
      "loss: 1.76653\n",
      "ITERATION: 59\n",
      "params:  {'max_depth': 10, 'max_features': 0.55, 'min_samples_leaf': 25}\n",
      "loss: 1.65396\n",
      "ITERATION: 60\n",
      "params:  {'max_depth': 9, 'max_features': 0.4, 'min_samples_leaf': 80}\n",
      "loss: 1.69516\n",
      "ITERATION: 61\n",
      "params:  {'max_depth': 4, 'max_features': 0.35000000000000003, 'min_samples_leaf': 85}\n",
      "loss: 1.95236\n",
      "ITERATION: 62\n",
      "params:  {'max_depth': 3, 'max_features': 0.05, 'min_samples_leaf': 40}\n",
      "loss: 2.36748\n",
      "ITERATION: 63\n",
      "params:  {'max_depth': 10, 'max_features': 0.55, 'min_samples_leaf': 75}\n",
      "loss: 1.66859\n",
      "ITERATION: 64\n",
      "params:  {'max_depth': 8, 'max_features': 0.2, 'min_samples_leaf': 25}\n",
      "loss: 1.76420\n",
      "ITERATION: 65\n",
      "params:  {'max_depth': 6, 'max_features': 0.30000000000000004, 'min_samples_leaf': 15}\n",
      "loss: 1.82943\n",
      "ITERATION: 66\n",
      "params:  {'max_depth': 10, 'max_features': 0.65, 'min_samples_leaf': 25}\n",
      "loss: 1.65517\n",
      "ITERATION: 67\n",
      "params:  {'max_depth': 10, 'max_features': 0.6000000000000001, 'min_samples_leaf': 25}\n",
      "loss: 1.65867\n",
      "ITERATION: 68\n",
      "params:  {'max_depth': 10, 'max_features': 0.45, 'min_samples_leaf': 25}\n",
      "loss: 1.66028\n",
      "ITERATION: 69\n",
      "params:  {'max_depth': 10, 'max_features': 0.65, 'min_samples_leaf': 25}\n",
      "loss: 1.65324\n",
      "ITERATION: 70\n",
      "params:  {'max_depth': 10, 'max_features': 0.55, 'min_samples_leaf': 50}\n",
      "loss: 1.66223\n",
      "ITERATION: 71\n",
      "params:  {'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 45}\n",
      "loss: 1.66146\n",
      "ITERATION: 72\n",
      "params:  {'max_depth': 2, 'max_features': 0.6000000000000001, 'min_samples_leaf': 70}\n",
      "loss: 2.14481\n",
      "ITERATION: 73\n",
      "params:  {'max_depth': 1, 'max_features': 0.75, 'min_samples_leaf': 25}\n",
      "loss: 2.37323\n",
      "ITERATION: 74\n",
      "params:  {'max_depth': 10, 'max_features': 0.45, 'min_samples_leaf': 95}\n",
      "loss: 1.67152\n",
      "ITERATION: 75\n",
      "params:  {'max_depth': 5, 'max_features': 0.35000000000000003, 'min_samples_leaf': 10}\n",
      "loss: 1.88662\n",
      "ITERATION: 76\n",
      "params:  {'max_depth': 9, 'max_features': 0.8, 'min_samples_leaf': 65}\n",
      "loss: 1.69059\n",
      "ITERATION: 77\n",
      "params:  {'max_depth': 7, 'max_features': 0.65, 'min_samples_leaf': 25}\n",
      "loss: 1.76347\n",
      "ITERATION: 78\n",
      "params:  {'max_depth': 10, 'max_features': 0.7000000000000001, 'min_samples_leaf': 30}\n",
      "loss: 1.65698\n",
      "ITERATION: 79\n",
      "params:  {'max_depth': 4, 'max_features': 0.9, 'min_samples_leaf': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.97259\n",
      "ITERATION: 80\n",
      "params:  {'max_depth': 10, 'max_features': 0.75, 'min_samples_leaf': 35}\n",
      "loss: 1.65642\n",
      "ITERATION: 81\n",
      "params:  {'max_depth': 8, 'max_features': 0.5, 'min_samples_leaf': 55}\n",
      "loss: 1.72499\n",
      "ITERATION: 82\n",
      "params:  {'max_depth': 3, 'max_features': 0.55, 'min_samples_leaf': 25}\n",
      "loss: 2.03429\n",
      "ITERATION: 83\n",
      "params:  {'max_depth': 2, 'max_features': 0.8500000000000001, 'min_samples_leaf': 60}\n",
      "loss: 2.14316\n",
      "ITERATION: 84\n",
      "params:  {'max_depth': 6, 'max_features': 0.7000000000000001, 'min_samples_leaf': 20}\n",
      "loss: 1.81626\n",
      "ITERATION: 85\n",
      "params:  {'max_depth': 10, 'max_features': 0.65, 'min_samples_leaf': 90}\n",
      "loss: 1.66931\n",
      "ITERATION: 86\n",
      "params:  {'max_depth': 1, 'max_features': 0.45, 'min_samples_leaf': 80}\n",
      "loss: 2.38894\n",
      "ITERATION: 87\n",
      "params:  {'max_depth': 10, 'max_features': 0.75, 'min_samples_leaf': 25}\n",
      "loss: 1.65401\n",
      "ITERATION: 88\n",
      "params:  {'max_depth': 5, 'max_features': 0.4, 'min_samples_leaf': 15}\n",
      "loss: 1.87141\n",
      "ITERATION: 89\n",
      "params:  {'max_depth': 7, 'max_features': 0.25, 'min_samples_leaf': 85}\n",
      "loss: 1.78589\n",
      "ITERATION: 90\n",
      "params:  {'max_depth': 9, 'max_features': 0.9, 'min_samples_leaf': 75}\n",
      "loss: 1.69132\n",
      "ITERATION: 91\n",
      "params:  {'max_depth': 10, 'max_features': 1.0, 'min_samples_leaf': 50}\n",
      "loss: 1.66448\n",
      "ITERATION: 92\n",
      "params:  {'max_depth': 4, 'max_features': 0.8, 'min_samples_leaf': 70}\n",
      "loss: 1.95941\n",
      "ITERATION: 93\n",
      "params:  {'max_depth': 3, 'max_features': 0.55, 'min_samples_leaf': 40}\n",
      "loss: 2.04658\n",
      "ITERATION: 94\n",
      "params:  {'max_depth': 8, 'max_features': 0.6000000000000001, 'min_samples_leaf': 45}\n",
      "loss: 1.72200\n",
      "ITERATION: 95\n",
      "params:  {'max_depth': 10, 'max_features': 0.65, 'min_samples_leaf': 25}\n",
      "loss: 1.65362\n",
      "ITERATION: 96\n",
      "params:  {'max_depth': 6, 'max_features': 0.9500000000000001, 'min_samples_leaf': 10}\n",
      "loss: 1.83266\n",
      "ITERATION: 97\n",
      "params:  {'max_depth': 2, 'max_features': 0.7000000000000001, 'min_samples_leaf': 30}\n",
      "loss: 2.14689\n",
      "ITERATION: 98\n",
      "params:  {'max_depth': 10, 'max_features': 0.8, 'min_samples_leaf': 95}\n",
      "loss: 1.66682\n",
      "ITERATION: 99\n",
      "params:  {'max_depth': 1, 'max_features': 0.8500000000000001, 'min_samples_leaf': 65}\n",
      "loss: 2.37216\n",
      "ITERATION: 100\n",
      "params:  {'max_depth': 10, 'max_features': 0.9, 'min_samples_leaf': 25}\n",
      "loss: 1.65465\n"
     ]
    }
   ],
   "source": [
    "trials = hyperopt.Trials()\n",
    "global  ITERATION\n",
    "ITERATION = 0\n",
    "best = hyperopt.fmin(\n",
    "    objective,\n",
    "    space=params_space,\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials,\n",
    "    rstate=RandomState(ITERATION)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = pd.read_csv('rf_trials_pfs4.csv')\n",
    "rf_results.sort_values('loss', ascending=True,inplace = True)\n",
    "rf_results.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>iteration</th>\n",
       "      <th>loss</th>\n",
       "      <th>params</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>1.652485</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 0.75, 'min_s...</td>\n",
       "      <td>617.687029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "      <td>69</td>\n",
       "      <td>1.653236</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 0.65, 'min_s...</td>\n",
       "      <td>532.923826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94</td>\n",
       "      <td>95</td>\n",
       "      <td>1.653618</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 0.65, 'min_s...</td>\n",
       "      <td>531.906359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>59</td>\n",
       "      <td>1.653962</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 0.55, 'min_s...</td>\n",
       "      <td>458.894292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>1.654012</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 0.75, 'min_s...</td>\n",
       "      <td>618.132501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  iteration      loss  \\\n",
       "0     31         32  1.652485   \n",
       "1     68         69  1.653236   \n",
       "2     94         95  1.653618   \n",
       "3     58         59  1.653962   \n",
       "4     86         87  1.654012   \n",
       "\n",
       "                                              params  train_time  \n",
       "0  {'max_depth': 10, 'max_features': 0.75, 'min_s...  617.687029  \n",
       "1  {'max_depth': 10, 'max_features': 0.65, 'min_s...  532.923826  \n",
       "2  {'max_depth': 10, 'max_features': 0.65, 'min_s...  531.906359  \n",
       "3  {'max_depth': 10, 'max_features': 0.55, 'min_s...  458.894292  \n",
       "4  {'max_depth': 10, 'max_features': 0.75, 'min_s...  618.132501  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = ast.literal_eval(rf_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'max_features': 0.75, 'min_samples_leaf': 25}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse with max_depth = 15 is 1.5890628824680237\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 50, max_depth = 15, max_features = best_params['max_features'], \n",
    "                               min_samples_leaf=best_params['min_samples_leaf'], n_jobs = -1)\n",
    "rf.fit(dict_cat['train'], dict_cat['train_y'])\n",
    "print('rmse with max_depth = 15 is {}'.format(error(rf.predict(dict_cat['val']), dict_cat['val_y'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse with max_depth = 20 is 1.5795238900710495\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 50, max_depth = 20, max_features = best_params['max_features'], \n",
    "                               min_samples_leaf=best_params['min_samples_leaf'], n_jobs = -1)\n",
    "rf.fit(dict_cat['train'], dict_cat['train_y'])\n",
    "print('rmse with max_depth = 20 is {}'.format(error(rf.predict(dict_cat['val']), dict_cat['val_y'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_months = np.arange(28,34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for month 28\n",
      "training for month 29\n",
      "training for month 30\n",
      "training for month 31\n",
      "training for month 32\n",
      "training for month 33\n"
     ]
    }
   ],
   "source": [
    "stack_x = []\n",
    "for i in stack_months:\n",
    "  data_train_x = full_data.loc[full_data['date_block_num']<i,:].copy()\n",
    "  data_test_x = full_data.loc[full_data['date_block_num']==i,:].copy()\n",
    "  data_train_x.drop('cnt_shop_item', axis = 1, inplace = True)\n",
    "  data_test_x.drop('cnt_shop_item', axis = 1, inplace = True)\n",
    "  data_train_y = full_data.loc[full_data['date_block_num']<i,'cnt_shop_item'].copy()\n",
    "  data_test_y = full_data.loc[full_data['date_block_num']==i,'cnt_shop_item'].copy()\n",
    "  print('training for month', i)\n",
    "  rf_stack = RandomForestRegressor(n_estimators = 50, max_depth = 20, max_features = best_params['max_features'], \n",
    "                                   min_samples_leaf=best_params['min_samples_leaf'], n_jobs = -1)\n",
    "  rf_stack.fit(data_train_x, data_train_y)\n",
    "  stack_x.extend(np.squeeze(rf_stack.predict(data_test_x)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Submission Time Series/Stacking/rf_train_level2.csv', stack_x, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "           max_features=0.75, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=25, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_x = pd.concat([dict_cat['train'], dict_cat['val']], axis=0)\n",
    "stack_y = pd.concat([dict_cat['train_y'], dict_cat['val_y']], axis = 0)\n",
    "rf_stack = RandomForestRegressor(n_estimators = 50, max_depth = 20, max_features = best_params['max_features'], \n",
    "                                 min_samples_leaf=best_params['min_samples_leaf'], n_jobs = -1)\n",
    "rf_stack.fit(stack_x, stack_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_level2_pred = rf_stack.predict(dict_cat['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_level2_pred_y = stack_level2_pred-1\n",
    "#stack_level2_pred_y = create_test_data(stack_level2_pred_y, dict_cat)\n",
    "np.savetxt('Submission Time Series/Stacking/rf_test_level2.csv', stack_level2_pred_y, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('Submission Time Series/Stacking/rf_final.pkl','wb') as handle:\n",
    "    pickle.dump(rf_stack,handle,protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
